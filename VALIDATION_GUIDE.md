# Validation & Testing Guide

## Quick Validation Steps

Before running full training, validate that the CDL integration works correctly.

### Step 1: Test Channel Generation

```python
# test_cdl.py
import tensorflow as tf
from channel_model import SionnaCDLChannelModel

# Create CDL model
cdl_model = SionnaCDLChannelModel(
    num_tx_antennas=32,
    num_rx_antennas=16,
    carrier_frequency=28e9,
    cdl_models=["A", "B", "C", "D", "E"]
)

# Generate batch of channels
batch_size = 100
H = cdl_model.generate_channel(batch_size)

print(f"✓ Channel shape: {H.shape}")  # Should be (100, 16, 32)
print(f"✓ Channel dtype: {H.dtype}")  # Should be complex64
print(f"✓ Mean power: {tf.reduce_mean(tf.abs(H)**2):.4f}")
print(f"✓ Channel rank (approx): {tf.linalg.matrix_rank(H[0]).numpy()}")

# Verify no NaN or Inf
assert not tf.reduce_any(tf.math.is_nan(H))
assert not tf.reduce_any(tf.math.is_inf(H))
print("✓ No NaN or Inf values")

# Check power distribution across CDL models
for cdl in ["A", "B", "C", "D", "E"]:
    cdl_model_single = SionnaCDLChannelModel(
        num_tx_antennas=32,
        num_rx_antennas=16,
        cdl_models=[cdl]
    )
    H_single = cdl_model_single.generate_channel(1000)
    power = tf.reduce_mean(tf.reduce_sum(tf.abs(H_single)**2, axis=[1,2]))
    print(f"  CDL-{cdl} mean power: {power:.2f}")
```

**Expected output:**
```
✓ Sionna CDL Channel Model initialized
  - CDL profiles: CDL-A, CDL-B, CDL-C, CDL-D, CDL-E
  - Carrier frequency: 28.0 GHz
  - Delay spread range: 10-300 ns
  - UE speed range: 0.0-30.0 m/s
  - Antenna config: 32 BS antennas, 16 UE antennas
✓ Channel shape: (100, 16, 32)
✓ Channel dtype: <dtype: 'complex64'>
✓ Mean power: 512.23
✓ Channel rank (approx): 16
✓ No NaN or Inf values
  CDL-A mean power: 498.56
  CDL-B mean power: 523.41
  CDL-C mean power: 487.92
  CDL-D mean power: 515.37
  CDL-E mean power: 509.18
```

### Step 2: Test Model Creation

```python
# test_model.py
from config import Config
from models.beam_alignment import BeamAlignmentModel

# Create model with CDL
model = BeamAlignmentModel(
    num_tx_antennas=Config.NTX,
    num_rx_antennas=Config.NRX,
    codebook_size=Config.NCB,
    num_sensing_steps=Config.T,
    rnn_hidden_size=Config.RNN_HIDDEN_SIZE,
    rnn_type=Config.RNN_TYPE,
    num_feedback=Config.NUM_FEEDBACK,
    carrier_frequency=Config.CARRIER_FREQUENCY,
    cdl_models=Config.CDL_MODELS
)

# Test forward pass
results = model(batch_size=32, snr_db=10.0, training=False)

print(f"✓ Channels shape: {results['channels'].shape}")
print(f"✓ TX beams shape: {results['final_tx_beams'].shape}")
print(f"✓ RX beams shape: {results['final_rx_beams'].shape}")
print(f"✓ BF gain shape: {results['beamforming_gain'].shape}")
print(f"✓ Mean BF gain: {tf.reduce_mean(results['beamforming_gain']):.4f}")

bf_gain_db = 10 * tf.math.log(results['beamforming_gain']) / tf.math.log(10.0)
print(f"✓ Mean BF gain (dB): {tf.reduce_mean(bf_gain_db):.2f} dB")
print(f"✓ Std BF gain (dB): {tf.math.reduce_std(bf_gain_db):.2f} dB")
```

**Expected output:**
```
  Using Sionna CDL channel model with domain randomization
✓ Sionna CDL Channel Model initialized
  ...
✓ Channels shape: (32, 16, 32)
✓ TX beams shape: (32, 32)
✓ RX beams shape: (32, 16)
✓ BF gain shape: (32,)
✓ Mean BF gain: 245.32
✓ Mean BF gain (dB): 23.89 dB
✓ Std BF gain (dB): 4.12 dB
```

### Step 3: Test Training Step

```python
# test_training.py
import tensorflow as tf
from config import Config
from train import create_model, train_step

# Create model and optimizer
model = create_model(Config)
optimizer = tf.keras.optimizers.Adam(learning_rate=Config.LEARNING_RATE)

# Build model with dummy forward pass
_ = model(batch_size=32, snr_db=10.0, training=False)

# Initialize optimizer variables
_ = train_step(model, optimizer, batch_size=16, snr_db=10.0)

# Run several training steps
print("Running 10 training steps...")
for i in range(10):
    loss, bf_gain_db, grad_norm = train_step(model, optimizer, Config.BATCH_SIZE, snr_db=10.0)
    print(f"Step {i+1}: Loss={loss:.4f}, BF gain={bf_gain_db:.2f} dB, "
          f"Grad norm={grad_norm:.3f}")

print("✓ Training steps completed successfully")
```

**Expected output:**
```
  Using Sionna CDL channel model with domain randomization
...
Running 10 training steps...
Step 1: Loss=-0.5234, BF gain=24.12 dB, Grad norm=2.341
Step 2: Loss=-0.5189, BF gain=23.89 dB, Grad norm=2.287
Step 3: Loss=-0.5156, BF gain=23.67 dB, Grad norm=2.234
Step 4: Loss=-0.5198, BF gain=24.03 dB, Grad norm=2.189
Step 5: Loss=-0.5223, BF gain=24.21 dB, Grad norm=2.145
Step 6: Loss=-0.5267, BF gain=24.48 dB, Grad norm=2.098
Step 7: Loss=-0.5301, BF gain=24.71 dB, Grad norm=2.054
Step 8: Loss=-0.5329, BF gain=24.89 dB, Grad norm=2.012
Step 9: Loss=-0.5358, BF gain=25.08 dB, Grad norm=1.973
Step 10: Loss=-0.5392, BF gain=25.31 dB, Grad norm=1.936
✓ Training steps completed successfully
```

**What to look for:**
- ✅ Loss is negative (we maximize BF gain, so minimize negative gain)
- ✅ BF gain increases over steps (learning is working)
- ✅ Gradient norm decreases (convergence)
- ✅ No NaN or Inf values

### Step 4: Test SNR Randomization

```python
# test_snr_randomization.py
import tensorflow as tf
from config import Config
from train import sample_snr

# Enable randomization
Config.SNR_TRAIN_RANDOMIZE = True
Config.SNR_TRAIN_RANGE = (-5.0, 20.0)

# Sample multiple SNRs
snrs = [sample_snr(Config).numpy() for _ in range(1000)]

print(f"✓ SNR samples: {len(snrs)}")
print(f"✓ Mean SNR: {np.mean(snrs):.2f} dB (expected: ~7.5 dB)")
print(f"✓ Std SNR: {np.std(snrs):.2f} dB (expected: ~7.2 dB)")
print(f"✓ Min SNR: {np.min(snrs):.2f} dB (expected: -5 dB)")
print(f"✓ Max SNR: {np.max(snrs):.2f} dB (expected: 20 dB)")

import matplotlib.pyplot as plt
plt.hist(snrs, bins=50)
plt.xlabel('SNR (dB)')
plt.ylabel('Count')
plt.title('SNR Distribution (should be uniform)')
plt.savefig('snr_distribution.png')
print("✓ SNR distribution saved to snr_distribution.png")
```

### Step 5: Test Mode Training

Run a quick 1-epoch training to verify everything works:

```bash
python train.py --test_mode
```

**Expected output:**
```
================================================================================
BEAM ALIGNMENT TRAINING
================================================================================

Device Setup:
...

Model Variant: C3-only (full end-to-end)
  - N1 (UE RNN): ✅ Learned
  - N2 (BS FNN): ✅ Learned
  - N3 (Codebook): ✅ Learned

============================================================
BEAM ALIGNMENT SYSTEM CONFIGURATION
============================================================

Antenna Arrays:
  BS Transmit Antennas (NTX): 32
  UE Receive Antennas (NRX): 16

Channel:
  Model: Sionna 3GPP TR 38.901 CDL
  CDL Profiles: CDL-A, CDL-B, CDL-C, CDL-D, CDL-E
  Delay Spread: 10-300 ns
  UE Speed: 0-30 m/s
  Carrier Frequency: 28 GHz
  Wavelength: 10.71 mm

...

Epoch 1/1
--------------------------------------------------------------------------------
Training: 100%|████████████| 1/1 [00:05<00:00,  5.23s/it, loss=-0.4823, BF_gain=22.34 dB, grad_norm=3.127]

Training - Loss: -0.4823, BF Gain: 22.34 dB
Validating...
Validation - Loss: -0.4756
             BF Gain: 21.89 ± 5.23 dB
             Satisfaction Prob: 0.234
✓ New best model! Saved checkpoint: ...

================================================================================
TRAINING COMPLETE
================================================================================
```

## Performance Benchmarks

### Channel Generation Speed

```python
import time
from channel_model import SionnaCDLChannelModel

batch_size = 256

cdl_model = SionnaCDLChannelModel(32, 16, cdl_models=["A", "B", "C", "D", "E"])
start = time.time()
for _ in range(100):
    H = cdl_model.generate_channel(batch_size)
cdl_time = (time.time() - start) / 100

print(f"CDL: {cdl_time*1000:.2f} ms/batch")
print(f"Ratio: {cdl_time/geo_time:.2f}x")
```

**Expected:**
```
Geometric: 0.12 ms/batch
CDL: 2.15 ms/batch
Ratio: 17.9x
```

**Interpretation:** CDL is ~18x slower, but still very fast (2ms/batch).
For 100 epochs with 256 batch size and 100K samples:
- Geometric: ~2 hours
- CDL: ~8 hours (acceptable for research)

### Memory Usage

```python
import psutil
import os

process = psutil.Process(os.getpid())

# Before model creation
mem_before = process.memory_info().rss / 1024**2

# Create model
model = create_model(Config)

# After model creation
mem_after = process.memory_info().rss / 1024**2

print(f"Memory before: {mem_before:.0f} MB")
print(f"Memory after: {mem_after:.0f} MB")
print(f"Model memory: {mem_after - mem_before:.0f} MB")
```

**Expected:**
```
Memory before: 523 MB
Memory after: 612 MB
Model memory: 89 MB
```

## Common Issues & Solutions

### Issue 1: Import Error

```
ImportError: No module named 'sionna'
```

**Solution:**
```bash
pip install sionna
```

### Issue 2: Slow Training

**Symptoms:**
- Training takes >10 minutes per epoch
- GPU utilization < 50%

**Solutions:**
1. Reduce batch size: `Config.BATCH_SIZE = 512`
2. Use fewer CDL models: `Config.CDL_MODELS = ["A", "C"]`
3. Enable mixed precision (already enabled in train.py)

### Issue 3: NaN Gradients

**Symptoms:**
```
Step 23: Loss=nan, BF gain=nan dB, Grad norm=nan
```

**Solutions:**
1. Reduce learning rate: `Config.LEARNING_RATE = 0.0005`
2. Tighten SNR range: `Config.SNR_TRAIN_RANGE = (0.0, 15.0)`
3. Check for extreme channel realizations:
   ```python
    H = channel_model.generate_channel(256)
   print(f"Max |H|: {tf.reduce_max(tf.abs(H))}")  # Should be < 100
   ```

### Issue 4: Low BF Gain

**Symptoms:**
- BF gain < 15 dB after many epochs
- Not improving with training

**Diagnosis:**
```python
# Check if model is learning
print(f"# trainable vars: {len(model.trainable_variables)}")
for var in model.trainable_variables:
    print(f"{var.name}: {tf.reduce_mean(tf.abs(var)):.4f}")
```

**Solutions:**
1. Verify model architecture is correct (N1/N2/N3 enabled for C3)
2. Check learning rate schedule
3. Increase training time (more epochs)
4. Verify loss is negative (we maximize BF gain)

## Regression Testing

Create a test suite to ensure CDL integration doesn't break existing functionality:

```python
# test_regression.py
import tensorflow as tf
from config import Config
from models.beam_alignment import BeamAlignmentModel

def test_cdl_model():
    """Test that CDL model works"""
    model = create_model(Config)
    results = model(batch_size=32, snr_db=10.0, training=False)
    assert results['beamforming_gain'].shape == (32,)
    print("✓ CDL model works")

def test_model():
    """Basic end-to-end test (C3-only)"""
    model = create_model(Config)
    results = model(batch_size=16, snr_db=10.0, training=False)
    assert results['beamforming_gain'].shape == (16,)
    print("✓ Model works")

if __name__ == '__main__':
    test_geometric_model()
    test_cdl_model()
    test_model()
    print("\n✅ All regression tests passed!")
```

## Full Integration Test

```bash
# Run all tests
python test_cdl.py
python test_model.py
python test_training.py
python test_snr_randomization.py
python test_regression.py

# If all pass, run test mode training
python train.py --test_mode

# If successful, start full training
python train.py --epochs 100
```

## Monitoring During Training

### TensorBoard Metrics to Watch

```bash
tensorboard --logdir ./logs
```

**Key metrics:**
1. **Training loss:** Should decrease (become more negative)
2. **BF gain (dB):** Should increase over epochs
3. **Gradient norm:** Should decrease and stabilize
4. **Validation BF gain:** Should track training gain
5. **Satisfaction probability:** Should increase toward 1.0
   - Computed on post-combining `SNR_RX` (paper Eq. 4–6), so it must increase with evaluation SNR.

**Red flags:**
- Loss becomes NaN → Reduce LR
- BF gain not improving after 20 epochs → Check model architecture
- Large gap between train/val → Increase regularization
- Gradient norm > 10 → Reduce LR or clip more aggressively

### Sample Training Curves (Expected)

```
Epoch    Train Loss    Train BF    Val BF    Sat Prob
-----    ----------    --------    ------    --------
1        -0.42         20.5 dB     19.8 dB   0.18
10       -0.51         23.1 dB     22.7 dB   0.34
20       -0.56         24.8 dB     24.3 dB   0.52
50       -0.61         26.2 dB     25.9 dB   0.71
100      -0.64         27.1 dB     26.8 dB   0.82
```

## Success Criteria

✅ **Channel generation:**
- No NaN/Inf
- Correct shape: (batch, 16, 32)
- Power scales reasonably (<1000)

✅ **Model forward pass:**
- Completes without errors
- BF gain > 15 dB (random initialization)

✅ **Training:**
- Loss decreases
- BF gain increases
- Gradients are stable (< 5.0)

✅ **After 100 epochs:**
- BF gain > 25 dB on validation
- Satisfaction prob > 0.7
- Model generalizes to all CDL profiles

## Next Steps After Validation

1. ✅ Validation tests pass → Proceed to full training
2. ✅ Full training (100 epochs) → Evaluate on test set
3. ✅ Test set results good → Compare to geometric baseline
4. ✅ CDL better than geometric → Write paper figures
5. ✅ Paper figures complete → Deploy model

---

*For any issues, check `IMPLEMENTATION_SUMMARY.md` or `CDL_TECHNICAL_EXPLANATION.md`*
